---
title: 'Machine Learning'
author: ""
date: ""
output: 
  html_document:
    toc: true
    toc_float: true
---


```{r include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, cache = TRUE, autodep = TRUE,
  fig.width = 7, fig.height = 5
)
```

## Goal

In this lab we will learn the basics of Machine Learning (ML). We will focus 
on supervised learning. We will start with the simple idea of linear 
discrimination, and then move on to important concepts in ML: cross-validation,
overfitting and variance-bias trade-off.

## Packages

Install packages.

```{r warning=FALSE, message=FALSE}
pkgs_needed = c("MASS","ExperimentHub", "tidyverse","glmnet",
                "RColorBrewer","caret", "magrittr")
letsinstall = setdiff(pkgs_needed, installed.packages()) 
if (length(letsinstall) > 0) {
  for (pkg in letsinstall) {
    BiocManager::install(pkg, dependencies = TRUE)
  }
}
```

Load packages.

```{r warning=FALSE, message=FALSE}
library("tidyverse")
library("MASS")
library("ExperimentHub")
library("glmnet")
library("RColorBrewer")
library("caret")
library("magrittr")
```


## Linear discrimination

We start with one of the simplest possible discrimination problems. Our aim is 
to partition the 2D plane into classes, using class boundaries that are straight 
lines.

### Diabetes data set

The `diabetes` dataset presents three different groups of diabetes
patients and five clinical variables measured on them.

```{r diabetes}
# load data
diabetes = read_csv(
  url("http://web.stanford.edu/class/bios221/data/diabetes.csv"), 
  col_names = TRUE
)
diabetes

# convert group to factor
diabetes$group <- factor(diabetes$group)
```

We converted values of `group` column from character into factor. If you are not
familiar with factors in R, you can read about them
[here](https://r4ds.hadley.nz/factors.html).  

Next we will visualize the data using `ggplot2` package. To plot all variables 
together, we need to transform the data into a long format, using `gather` 
function from `tidyr` package.

```{r ldagroups}
# transform data to long format
diabetes.long = gather(diabetes, variable, value, -c(id,group))

# plot distribution of different variables in the data
ggplot(data = diabetes.long, mapping = aes(x = value, col = group)) +
  geom_density() + facet_wrap( ~ variable, ncol = 2, scales = "free") 
```

We see already from the one-dimensional distributions that some of the
individual variables could potentially predict which group a patient is more 
likely to belong to. Our goal will be to combine variables to improve these 
one dimensional predictions.
  
### LDA

Let's see whether we can predict the `group` from the `insulin` and `glutest`
variables in the diabetes data. It's always a good idea to first visualize the 
data, and here we look at how insulin and glutest are distributed in the data 
relative to each other.

```{r scatterdiabetes}
ggdb = ggplot(diabetes, aes(x = insulin, y = glutest, colour = group)) +
         geom_point()
ggdb
```

We'll start with a method called linear discriminant analysis (LDA). 
This method is a foundation stone of classification, many of the more 
complicated (and sometimes more powerful) algorithms are really just 
generalizations of LDA.

To calculate LDA, we will use the `lda` function from the `MASS` package.

```{r ldafit}
# fit lda model
diabetes_lda = lda(group ~ insulin + glutest, data = diabetes) 
diabetes_lda
```

We can then use the model for prediction.

```{r ldaresults}
# predictions from model
ghat = predict(diabetes_lda)$class

# contingency table
table(predicted = ghat, truth = diabetes$group)

# classification error
mean(ghat != diabetes$group)
```

Now, let's visualize the LDA result. We are going to plot the prediction regions 
for each of the three groups. We do this by creating a grid of points and using
our prediction rule on each of them. We'll then also dig a bit deeper into the 
mechanics of LDA and plot the class centers. Assembling this visualization 
requires us to write a bit of code.

```{r make1Dgrid}
# helper function to make 1D grid
make1Dgrid = function(x) {
  # extend the range of x by small fraction on both sides
  rg = grDevices::extendrange(x)
  # generate 100 sequential values within extended range of x
  seq(from = rg[1], to = rg[2], length.out = 100)
}
```

Set up the points for prediction, a $100 \times 100$ grid that covers the data 
range.

```{r diabetes_grid}
# get all combinations of insulin and glutest in the range of the data
diabetes_grid = with(diabetes,
  expand.grid(insulin = make1Dgrid(insulin),
              glutest = make1Dgrid(glutest)))
```

Do the predictions.

```{r diabetes_grid2}
diabetes_grid$ghat = predict(diabetes_lda, newdata = diabetes_grid)$class
```

The group centers.

```{r centers}
centers = diabetes_lda$means
```

Now we are ready to plot:

```{r modeldiabetes, fig.width = 5, fig.height = 4}
# we start from scatterplot of the data used for training the model
ggdb + 
  # we add prediction regions plotted as raster image on top of original points
  geom_raster(
    data = diabetes_grid,
    mapping = aes(x = insulin, y = glutest, fill = ghat),
    inherit.aes = FALSE, alpha = 0.25, interpolate = TRUE
  ) +
  # we add centers as + points
  geom_point(
    data = as_tibble(centers),
    mapping = aes(x = insulin, y = glutest),
    inherit.aes = FALSE, pch = "+", size = 8
  ) +
  scale_x_continuous(expand = c(0, 0)) + 
  scale_y_continuous(expand = c(0, 0))
```

Instead of trying to predict with 2 variables, we could use LDA with all 
5 variables.

```{r}
diabetes_lda5 = lda(group ~ relwt + glufast + glutest +
           steady + insulin, data = diabetes)
diabetes_lda5
```

Let us get the predictions:

```{r}
ghat5 = predict(diabetes_lda5)$class
table(ghat5, diabetes$group)
```

And the prediction error:

```{r}
mean(ghat5 != diabetes$group)
```

The prediction error is almost half compared to what we got with only the 
insulin and glutest variables. Is this because indeed the classifier based on
5 variables is better, or are we just overfitting our dataset? Cross-validation 
in the next section will provide one possible answer to this question.

## Cross-validation

Previously we were looking at predictions and classification error on the same 
data as the model was trained on. This is not a good idea, as the model will 
always perform better on the training data than on new data. In extreme cases, 
model doesn't learn the trends underlying the data, but simply memorizes the 
values it is trained on - this is called overfitting, and is something we want 
to make sure we avoid.

To get a good idea if the model learns well, we need to estimate its performance
on new data it hasn't seen before. The most simple way to do this, we would to 
use a fraction of data only for training, and a fraction only for testing. 
However, by splitting the data only once, our results can be quite affected by 
how the split happens to fall. Another way to deal with this would be to perform
cross validation (CV).

Below we will look at one version of CV, called leave-one-out cross-validation 
(LOOCV). In LOOCV, we fit the whole model without one data point and then we 
predict the label of the left-out point. We know the true label of that data 
point, and the learning algorithm does not, hence we can get a reasonable 
estimate of the algorithm's performance on this "new" data.  Then, this process 
is repeated for each datapoint separately, and the average error is calculated.

Let's write a function which does this:

```{r}
estimate_mcl_loocv = function(x, resp) {
  vapply(seq_len(nrow(x)), function(i) {
    fit  = lda(x[-i, ], resp[-i])
    ptrn = predict(fit, newdata = x[-i,, drop = FALSE])$class
    ptst = predict(fit, newdata = x[ i,, drop = FALSE])$class
    c(train = mean(ptrn != resp[-i]), test = (ptst != resp[i]))
  }, FUN.VALUE = numeric(2)) %>% rowMeans %>% t %>% as_tibble
}
```

Now let's try a simulation: 

We generate artificial "apple" and "orange" data and measure $p$ predictors on 
them. The predictors are constructed in such a way that the first 6 are 
informative (i.e. apples and oranges are different with respect to these), 
but the rest are just random noise. We want to try different models where we
include the first $k$ predictors. We will use LOOCV to find $k$. Note that 
from the simulation setting we know we should pick $k=6$ but LOOCV does not
know this:

(Remark: The code below might take a while to run -- indeed one of the 
disadvantages of LOOCV is its long run time. If it takes too long you can 
try reducing the replications from 100 to a small number. In fact, it might 
be interesting to reduce the replications to 1 since with real datasets you 
don't have the luxury of repeating the data generation!)


```{r curseofdim, warning = FALSE}
n = 20
p = 2:20

# This will take a while
mcl = replicate(10, {
  xmat = matrix(rnorm(n * last(p)), nrow = n)
  resp = sample(c("apple", "orange"), n, replace = TRUE)
  xmat[, 1:6] = xmat[, 1:6] + as.integer(factor(resp))

  lapply(p, function(k) {
    estimate_mcl_loocv(xmat[, 1:k], resp)
  }) %>% bind_rows %>% cbind(p = p) %>% gather(variable, value, -p)
}, simplify = FALSE) %>% bind_rows 

mcl =  group_by(mcl, p, variable) %>%
   summarise(value = mean(value))

ggplot(mcl, aes(x = p, y = value, col = variable)) + geom_line() +
      geom_point() + ylab("Misclassification rate")
```

In the section below you will see CV used to pick a parameter for LASSO-logistic 
regression.

## Variance-bias trade-off

### Metagenome data set

Zeller et al. (2014) studied metagenome sequencing data from fecal samples of 156 
humans that included colorectal cancer patients and tumor-free controls. Their 
aim was to see whether they could identify biomarkers (presence or abundance 
of certain taxa) that could help with early tumor detection. The data are 
available from [Bioconductor](https://www.bioconductor.org) through its 
`ExperimentHub` service under the identifier EH359.

```{r colon1, results = "hide"}
library("ExperimentHub")
eh = ExperimentHub()
zeller = eh[["EH361"]]
```

```{r colon1b}
table(zeller$disease)
```

If you want, you can explore the `eh` object to see what other datasets there are.

For the following, let's focus on the normal (`n`) and `cancer` samples, and set the 
adenomas aside.

```{r colon2}
zellerNC = zeller[, zeller$disease %in% c("n", "cancer")]
```

Before jumping into model fitting, as always it's a good idea to do some
exploration of the data. First, let's look at the sample annotations.

```{r zellerpData}
head(pData(zellerNC))
tail(pData(zellerNC))
```

Next, let's explore the feature names.
  
```{r zellerpData_end}
head(rownames(zellerNC))
tail(rownames(zellerNC))
```

As you can see, the features are a mixture of abundance quantifications at 
different taxonomic levels, from **k**ingdom over **p**hylum to **s**pecies.
We could select only some of these, but here we continue with all of them.  

Next, let's look at the distribution of some of the features. Here, we show 
an arbitrary choice of two (510th and, 527th); in practice, it is helpful 
to scroll through many such plots quickly to get an impression.

```{r zellerHist}
ids <- c(510, 527)
tidy_zeller_subset <- as.data.frame(t(exprs(zellerNC)[ids, ])) %>% 
               mutate(Var2 = colnames(exprs(zellerNC))) %>% 
               gather(Var1, value, - Var2)

ggplot(tidy_zeller_subset, aes(x = value)) +
    geom_histogram(bins = 25) +
    facet_wrap( ~ Var1, ncol = 1, scales = "free")
```

### GLM with L1 regularisation (LASSO)

In the simplest case, we fit model

$$
\log \frac{P(Y=i\,|\,X=x)}{P(Y=k\,|\,X=x)} = \beta^0_i + \beta_i^\top x
$$

as follows.

```{r glmnet}
library("glmnet")
glmfit = glmnet(x = t(exprs(zellerNC)),
                y = factor(zellerNC$disease),
                family = "binomial")
```

A remarkable feature of the `glmnet` function is that it fits the model not 
only for one choice of $\lambda$, but for all possible $\lambda$s at once. 
For now, let's look at the prediction performance for, say, $\lambda=0.04$.
The name of the function parameter is `s`:

```{r colonPred}
pred = predict(glmfit, newx = t(exprs(zellerNC)), type = "class", s = 0.04)
confusion_table = table(predicted = pred, truth = zellerNC$disease)
confusion_table
```

**Quiz questions 1 and 2**: What is the true positive rate (TPR) and true
negative rate (SPC)? Hint: See book or 
[Wikipedia](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).

**Answer:**

```{r}

```

```{r}  

```
  
Not bad. But remember that this is on the training data, without 
cross-validation.  

Let's have a closer look at `glmfit`. The `glmnet` package offers a diagnostic 
plot that is worth looking at:

```{r plotglmfit}
plot(glmfit, xvar = "norm", col = RColorBrewer::brewer.pal(12, "Set3"), lwd = sqrt(3))
```

What is the x-axis? What are the different lines? Check the ``plot.glmnet``
documentation and look at different `xvar` options.

In particular, we note that as the penalty $\lambda$ increases, the L1 Norm 
of the coefficients ($\sum |\beta_i|$) shrinks. `glmnet` only shrinks the 
coefficients corresponding to the variables and not the intercept, however for 
simplicity our computations (and quiz questions) below also include the intercept 
(the first coefficient returned from `coef`).

For example as above let's see what fitted coefficients we got for 
$\lambda = 0.04$:

```{r}
fitted_beta = coef(glmfit, s=0.04)
sum(abs(fitted_beta))
```

Let's try with larger $\lambda$:

```{r}
sum(abs(coef(glmfit, s=0.1)))
```

**Quiz question 3**: Inspect the `glmfit` object: For how many different 
values of $\lambda$ did `glmnet` fit the model using settings as above?

**Answer:**
    
```{r}

```

**Quiz question 4**: For each of the values of $\lambda$ in the object above, 
calculate the L1 Norm (as we did above for two values of $\lambda$). 
Which $\lambda$ most closely corresponds to a L1 norm 6000?

**Answer:** 

```{r}

```

**Quiz question 5**: How many non-zero coefficients do you get for 
the $\lambda$ that you found in the previous question?

```{r}

```

To choose the best regularization parameter $\lambda$, we use cross-validation.

```{r colonCV}
set.seed(0xdada2)
cvglmfit = cv.glmnet(x = t(exprs(zellerNC)),
                     y = factor(zellerNC$disease),
                     family = "binomial")
plot(cvglmfit)
cvglmfit$lambda.min
cvglmfit$lambda.1se
```

We can access the optimal value with:

```{r lambda.min}
cvglmfit$lambda.min
```

As this value results from finding a minimum in an estimated curve, it turns out 
that it is often too small, i.\,e., that the implied penalization is too weak. 
A heuristic recommended by the authors of the `glmnet` package is to use a 
somewhat larger value instead, namely the largest value of $\lambda$ such that
the performance measure is within 1 standard error of the minimum.

```{r lambda.1se}
s0 = cvglmfit$lambda.1se
s0
```

**Quiz question 6**: How does the confusion table look like for $\lambda=$ 
lambda.1se ? Report the top left element of the confusion table 
(i.e. number of correctly classified cancer samples). Hint: Use function
`predict`.

**Answer**:

```{r}

```

**Quiz question 7**: What features drive the classification (at $\lambda$ = `lambda.1se` 
chosen by cross-validation with the 1 standard error rule)? 
Report the top one (the one with the largest absolute value of its coefficient).

**Answer**:

```{r}

```

## Method hacking

We encountered p-value hacking. A similar phenomenon exists in statistical 
learning: given a dataset, we explore various different methods of preprocessing 
(such as normalization, outlier detection, transformation, feature selection), 
try out different machine learning algorithms and tune their parameters until 
we are content with the result. The measured accuracy is likely to be too 
optimistic, i.e., will not generalize to a new dataset. Embedding as many of 
our methodical choices into a computational formalism and having an outer 
cross-validation loop (not to be confused with the inner loop that does the 
parameter tuning) will ameliorate the problem. But is unlikely to address it 
completely, since not all our choices can be formalized.

The gold standard remains validation on truly unseen data. In addition, it is
never a bad thing if the classifier is not a black box but can be interpreted 
in terms of domain knowledge. Finally, report not just summary statistics, 
such as misclassification rates, but lay open the complete computational 
workflow, so that anyone (including your future self) can convince themselves
of the robustness of the result or of the influence of the preprocessing, 
model selection and tuning choices.

After this word of caution, have a look at the `caret` package. It contains 
[a large number of machine learning methods](https://topepo.github.io/caret/available-models.html)
with an common interface. 

```{r caret1, message = FALSE}
library("caret")
caretMethods = names(getModelInfo())
head(caretMethods, 8)
```

**Quiz question 8**: How many methods does `caret` currently include?

```{r}

```
